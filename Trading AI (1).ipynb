{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "887fd21d-292d-433e-8f09-a0519cac7a9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Portfolio Optimisation AI\n",
    "\n",
    "Portfolio Optimisation AI uses deep reinforcement learning to actively control A portfolio of input stocks, cryptocurrencies to optimize portfolio risk and return. Deep reinforcment learning is used to maximise porfolio's logarithmic return by 'learning' to mitigate risk and improve portfolio sortano and sharpe ratios.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48356ada-4bb7-4513-8a14-77aee2fa8130",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Methods\n",
    "\n",
    "G is global array of previous timeseries data of m selected stocks and is used as the input data for the model. First row contains the 'risk free' asset and hence has constant value 1. all other timeseries price data is normalised to the price of rick free asset at that time. \n",
    " $$ G =\n",
    "  \\left[ {\\begin{array}{cccc}\n",
    "    1 & 1 & \\cdots & 1\\\\\n",
    "    x_{21} & x_{22} & \\cdots & x_{2T}\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "    x_{m1} & x_{m2} & \\cdots & x_{mT}\\\\\n",
    "  \\end{array} } \\right]$$\n",
    "The proportion of capital invested in each stock is contained in vector w_t.\n",
    "\n",
    "The states of the envirment $S_t$ used as input into networks is given by: \n",
    " $$ S_t =\n",
    "  \\left[ {\\begin{array}{cccc}\n",
    "    1 & 1 & \\cdots & 1\\\\\n",
    "    \\frac{x_{2,t-w+1}}{x_{2,t}} & \\frac{x_{2,t-w+1}}{x_{2,t}} & \\cdots & 1\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{x_{m,t-w+1}}{x_{m,t}} & \\frac{x_{2,t-w+1}}{x_{m,t}} & \\cdots & 1\\\\\n",
    "  \\end{array} } \\right]$$\n",
    "  \n",
    "The return of stock in a portfolio is defined by the ratio of the closing price over the opening price of the stock for given trading peroid. This can be calculated by \n",
    "$$w_t\\cdot y_t$$ where y_t is price change vector overperiod t ,\n",
    "$$y_t = \\left[\\begin{array}{cccc}\\frac{x_{1,t}}{x_{1,t-1}} &\\frac{x_{2,t}}{x_{2,t-1}}& \\cdots &\\frac{x_{m,t}}{x_{m,t-1}}\\\\\\end{array}\\right] $$\n",
    "The return of portfolio after some time T given by: \n",
    "$$ R_t = \\prod_{t=1}^Tr_t(1-c\\sum_{i=1}^m|w_{t,i}-w_{t-1,i}|)$$\n",
    "\n",
    "The reward use by the algorithm is calulated as:\n",
    "$$ \\sum_{t=1}^T\\ln\\mu_tw_t\\cdot y_t  $$\n",
    "where $\\mu$ is the factor of which of which portfolio shrinks by due to transaction costs. \n",
    "$\\mu $ is calculated iteratively by:\n",
    "$$ \\mu_t = \\frac{1}{1-C_pw_{t,0}}\\left(1-C_pw_{0,t}-C_s+C_p-C_sC_p)\\sum_1^m(w_{t,i}-\\mu_t{w_t,i})^+   \\right)  $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfafae13-e21a-4173-94f1-3451794834e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "$\\mathbf{\\text{Training Algorithm: 'On policy' PPO style }}$<br>\n",
    "***\n",
    "1.&emsp;Initialize policy network $\\mu$ with weights $\\theta^{\\mu}$ and Critic network $Q(S_t,a|\\theta^{\\mu})$ with weights $\\theta^{Q}$ and trajectory buffer $R_n$.\n",
    "\n",
    "2.&emsp;For t = 1 to T:<br>\n",
    "  &emsp;&emsp;If $\\mod{t} = Size(R_n)$\n",
    "  \n",
    "&emsp;&emsp;(a)&emsp;Use policy network to produce the action mean $ \\textbf{m}(S_t)\\in \\mathbb{R}^n$ and standar deviation $ \\boldsymbol{\\sigma}(S_t) \\in \\mathbb{R}^{n}$\n",
    "\n",
    "&emsp;&emsp;(b) execute action $a_t \\sim \\mathcal{N}(\\textbf{m},\\,\\boldsymbol{\\sigma})$\n",
    "calculate $p_t = \\ln(P(a_t)) $ and $Q(a_t,S_t)$\n",
    "\n",
    "&emsp;&emsp;(c) Solve the optimization problem (1) for the one step greedy\n",
    "action $\\bar{a}$\n",
    "\n",
    "&emsp;&emsp;(d) Store trajectory $(S_t,a_t,r_t,p_t,Q_t)$ in $R_n$\n",
    "\n",
    "3.&emsp; Sample random batches from  $R_n$\n",
    "\n",
    "  &emsp; calculate probabitlity ratio for batch \\$$R(\\theta^\\mu)= \\frac{\\mu_{\\theta^{\\mu}}(S_t,a_t)}{ \\mu_{\\theta^{\\mu} old}(S_t,a_t)} $$\n",
    "  \n",
    "  &emsp;&emsp;estimate value at t  $Q(s_t,a_t)$ and calulate the advantage $$\\hat{A}_t = \\sum_{\\tau>t}\\gamma^{\\tau-t}\\lambda_\\tau - Q(s_t,a_t)$$ \n",
    "  \n",
    "  calulate loss $$\\boldsymbol{\\nabla}_{\\theta^\\mu}\\frac{1}{B}\\sum_{t=1}^N\\sum_{b=1}^B \\min\\left(\\frac{\\mu_{\\theta^{\\mu}}(S_t,a_t)}{ \\mu_{\\theta^{\\mu} old}(S_t,a_t)},clip\\left(\\frac{\\mu_{\\theta^{\\mu}}(S_t,a_t)}{ \\mu_{\\theta^{\\mu} old}(S_t,a_t)},1-\\eta,1+\\eta\\right)\\right)$$\n",
    "  \n",
    "  calculate loss $$\\boldsymbol{\\nabla}_{\\theta^\\mu}\\frac{1}{B}\\sum_{t=1}^N \\hat{A}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e207dff1-b6bd-4890-8cdc-17bfc6dbfe50",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb52b1-9b21-47df-a6a6-f46654f4a0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37676e6a-5262-4984-aae0-d20b8bfb0a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17c53cc4-f104-48ac-99f5-e34aa760ad31",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c4de5-e46e-44dd-b66a-aa13f2a6b372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
